MFE stands for Micro Frontend â€” a key concept in modern Open edX architecture.

â¸»

ğŸ§± What is an MFE (Micro Frontend)?

A Micro Frontend (MFE) is a modular, React-based frontend app that handles a specific function or page within the Open edX platform â€” such as the learner dashboard, course home, profile page, or account settings.

Instead of one massive frontend app, Open edX breaks up the user interface into multiple independently deployed, maintainable apps â€” each an MFE.

â¸»

ğŸ“¦ Examples of Open edX MFEs

MFE App Name	Function	Path
account	User profile and settings	/account
authn	Login, registration, password reset	/login, /register
dashboard	Learner dashboard	/dashboard
gradebook	Instructor gradebook	/gradebook
learning	Learner course experience	/courses/:id
profile	Public profile view	/u/username
course-authoring	Course outline, settings (for Studio)	/course-authoring/...
frontend-app-ora	Open Response Assessment support (peer grading)	

Each of these lives in a separate GitHub repo (e.g., frontend-app-learning) and runs independently but communicates with the Open edX LMS or Studio backend.

â¸»

ğŸ’¡ Why MFEs Matter for You

Since youâ€™re building custom features like AI tutoring, MFEs are powerful because:

Benefit	Description
ğŸ§© Modularity	You can build, test, and deploy only the part you change (e.g., AI widget).
âš¡ Performance	Faster page loads and easier frontend optimization.
ğŸ¨ Custom UIs	Build rich, React-based user interfaces â€” not limited to legacy templates.
ğŸ”Œ Integration Friendly	Easily integrate with REST APIs, including your AI backend.
ğŸš€ Future-Proof	Open edX is migrating fully to MFEs. Youâ€™re building on the future path.



â¸»

ğŸ”§ How MFEs Run with Tutor

Tutor supports MFEs via plugins. For example:

tutor plugins enable mfe

It hosts them at:
	â€¢	http://local.edly.io/account
	â€¢	http://local.edly.io/dashboard
	â€¢	etc.

You can:
	â€¢	Customize an existing MFE (clone & modify)
	â€¢	Build your own (e.g., frontend-app-ai-tutor)
	â€¢	Add routes and integrate with your backend API

â¸»

ğŸ”¨ Want to Build Your Own MFE?

I can help you:
	â€¢	Scaffold a new Open edX-compatible MFE
	â€¢	Connect it to your AI FastAPI backend
	â€¢	Deploy it in Tutor alongside LMS/Studio

____

Awesome! Hereâ€™s a complete guide to scaffold your own Open edX-compatible Micro Frontend (MFE) â€” perfect for adding custom features like an AI Tutor dashboard, insights panel, or admin tools.

â¸»

ğŸ› ï¸ Step-by-Step: Scaffold a New MFE for Open edX

Weâ€™ll create a React-based MFE, similar to existing Open edX MFEs, and integrate it with Tutor for local development.

â¸»

ğŸ§° Prerequisites
	â€¢	Node.js (LTS)
	â€¢	Yarn
	â€¢	Tutor installed (pip install tutor)
	â€¢	Docker & Docker Compose

â¸»

ğŸ“ Step 1: Clone the Open edX MFE Starter

Use the official frontend template:

git clone https://github.com/openedx/frontend-template-application frontend-app-ai-tutor
cd frontend-app-ai-tutor



â¸»

ğŸ§± Step 2: Customize Your App
	1.	Change the app name in package.json:

{
  "name": "frontend-app-ai-tutor",
  "version": "0.1.0",
  ...
}


	2.	Change route path (in src/index.jsx):

import App from './App';
ReactDOM.render(<App />, document.getElementById('root'));


	3.	Update src/App.jsx to something custom:

import React from 'react';

const App = () => {
  return (
    <div>
      <h1>Welcome to the AI Tutor!</h1>
      <p>This MFE will deliver smart learning assistance using GenAI.</p>
    </div>
  );
};

export default App;



â¸»

ğŸ§ª Step 3: Test It Locally (Optional)

Run it locally:

yarn install
yarn start

Visit: http://localhost:8080

â¸»

ğŸ”Œ Step 4: Add Your MFE to Tutor

1. Create a plugin for your MFE

tutor plugins new mfe-ai-tutor

This creates a folder like:

$(tutor plugins printroot)/mfe-ai-tutor/

2. Copy your MFE source into the plugin

Inside your plugin, create:

mfe-ai-tutor/
â””â”€â”€ build/
    â””â”€â”€ ai-tutor/  <-- Copy your compiled MFE here

Use yarn build from your frontend folder to generate production files.

3. Edit plugin.py

from tutor import hooks

hooks.Filters.MFE_APPS.add_items([
    ("ai-tutor", "ai-tutor", "http://localhost:1999"),
])

Or for static hosting:

hooks.Filters.MFE_HOSTS.add_items([
    ("ai-tutor", "/mfe/ai-tutor")
])



â¸»

ğŸŒ Step 5: Rebuild Tutor with MFE
	1.	Enable your plugin:

tutor plugins enable mfe-ai-tutor


	2.	Mount and serve your MFE:

tutor images build mfe
tutor local start -d



Now visit:

http://local.edly.io/mfe/ai-tutor

Or whatever URL you configured.

â¸»

ğŸ“¡ Step 6: Integrate with Your AI Backend

Inside your MFE (React), use:

fetch("http://ai-backend:8000/tutor/assist", {
  method: "POST",
  headers: {
    Authorization: `Bearer ${accessToken}`,
    "Content-Type": "application/json"
  },
  body: JSON.stringify({ question: "What is Newton's second law?" })
})

You can fetch the accessToken from Open edX auth if needed.

â¸»

âœ… Summary

What You Did	Result
Scaffolded frontend	React app using Open edX MFE template
Integrated with Tutor	Served via mfe-ai-tutor plugin
Connected to AI	Ready to call your FastAPI backend



â¸»

Awesome! Letâ€™s walk through how to connect an Open edX Micro Frontend (MFE) to your AI FastAPI backend. This setup allows you to build a React-based frontend (as an MFE) where learners ask questions and get real-time answers from your AI tutor.

â¸»

ğŸ¯ Goal
	â€¢	Build a custom MFE (e.g., frontend-app-ai-tutor)
	â€¢	Connect it to your AI FastAPI backend
	â€¢	Secure the connection with JWT tokens from Open edX (Azure B2C or LMS auth)
	â€¢	Run it within the Tutor ecosystem

â¸»

ğŸ§± Architecture Overview

Learner (browser)
   â”‚
   â–¼
[Custom MFE (React)]  â”€â”€â”€â”€â”€â”€â–¶ [AI FastAPI Backend]
   â”‚                         â†³ Auth via JWT
   â–¼
[LMS (Tutor)] â† Token issuer (Azure B2C / LMS OIDC)



â¸»

ğŸ§° Step-by-Step Integration Guide

âœ… 1. Scaffold a Custom MFE

git clone https://github.com/openedx/frontend-template-application frontend-app-ai-tutor
cd frontend-app-ai-tutor
make install

Set app name and routes in .env and src/index.jsx.

â¸»

ğŸ§ª 2. Build the AI Tutor UI in React

Inside src/components/AITutor.jsx:

import React, { useState, useEffect } from 'react';
import { getAuthenticatedUser } from '@edx/frontend-platform/auth';

const AITutor = () => {
  const [question, setQuestion] = useState('');
  const [answer, setAnswer] = useState('');
  const [token, setToken] = useState('');

  useEffect(() => {
    getAuthenticatedUser().then(user => {
      setToken(user.accessToken);
    });
  }, []);

  const askQuestion = async () => {
    const res = await fetch('http://localhost:8001/tutor/assist', {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({ question })
    });
    const data = await res.json();
    setAnswer(data.reply);
  };

  return (
    <div className="ai-tutor">
      <h2>Ask the AI Tutor</h2>
      <input
        value={question}
        onChange={(e) => setQuestion(e.target.value)}
        placeholder="Ask a course question..."
      />
      <button onClick={askQuestion}>Submit</button>
      <p><strong>Answer:</strong> {answer}</p>
    </div>
  );
};

export default AITutor;

Then include <AITutor /> in your main page or route.

â¸»

ğŸ” 3. Secure API Calls with JWT Token

Use the @edx/frontend-platform/auth package to get the current authenticated userâ€™s JWT access token. This token is signed by Open edX (or Azure B2C if integrated) and passed in the header to your FastAPI backend.

â¸»

âš™ï¸ 4. Configure FastAPI Backend to Accept Token

In your FastAPI backend (auth.py):

from jose import jwt
from fastapi import Header, HTTPException
from pydantic import BaseModel

class User(BaseModel):
    email: str
    name: str
    username: str

def get_current_user(authorization: str = Header(...)) -> User:
    token = authorization.replace("Bearer ", "")
    payload = jwt.decode(token, options={"verify_signature": False})  # Dev only!
    return User(
        email=payload.get("email", ""),
        name=payload.get("name", ""),
        username=payload.get("sub", "")
    )



â¸»

ğŸš€ 5. Serve Your MFE via Tutor

In your custom MFE repo:

make build

Then host it using the mfe plugin in Tutor:

tutor config save --set MFE_APP_AI_TUTOR_MFE_REPO=https://github.com/your-org/frontend-app-ai-tutor
tutor config save --set MFE_APP_AI_TUTOR_BASE_URL="http://local.edly.io/ai-tutor"
tutor plugins enable mfe
tutor images build mfe
tutor local start -d

Or mount your dev build in Docker:

services:
  mfe:
    volumes:
      - ./dist:/openedx/dist/ai-tutor

Now you can access your custom MFE at:

http://local.edly.io/ai-tutor



â¸»

ğŸ§  Bonus: Add Route to LMS/Studio Navigation

Customize the LMS theme or use a plugin to add a nav item pointing to:

/ai-tutor



â¸»

âœ… Summary

Step	Tool
Frontend (MFE)	React + Open edX MFE template
Auth integration	@edx/frontend-platform/auth
Backend AI service	FastAPI + OpenAI/Claude
Secure token passing	JWT via Authorization header
Deployment with Tutor	MFE plugin + Docker Compose



â¸»

Excellent! Hereâ€™s a step-by-step guide to build and deploy your own custom MFE (Micro Frontend) â€” for example, an AI Tutor app â€” alongside the LMS and Studio using Tutor.

â¸»

ğŸ¯ Goal

Youâ€™ll create and deploy a custom React-based MFE, called frontend-app-ai-tutor, served at:

https://yourdomain/ai-tutor

It will:
	â€¢	Run via Tutor
	â€¢	Communicate with your FastAPI AI backend
	â€¢	Follow Open edX MFE conventions

â¸»

âœ… Prerequisites

Make sure you have:
	â€¢	Tutor â‰¥ 14 installed (pip install tutor)
	â€¢	MFE plugin enabled: tutor plugins enable mfe
	â€¢	Node.js (v18+) and Yarn (npm i -g yarn)
	â€¢	Docker installed and running

â¸»

ğŸ§± Step 1: Scaffold Your Custom MFE

Option 1: Clone and Customize an Existing MFE

git clone https://github.com/openedx/frontend-app-profile frontend-app-ai-tutor
cd frontend-app-ai-tutor
rm -rf .git

Option 2: Create a React App from Scratch (if going fully custom)

npx create-react-app frontend-app-ai-tutor --template typescript

Then add:
	â€¢	Routing (React Router)
	â€¢	Open edX config structure (see below)
	â€¢	API integration with your AI backend

â¸»

ğŸ›  Step 2: Add Open edX MFE Config

Create a file public/config.json:

{
  "BASE_URL": "/ai-tutor",
  "API_BASE_URL": "http://localhost:8001",  // Your FastAPI backend
  "PLATFORM_NAME": "Open edX",
  "SUPPORT_SITE_LINK": "/support"
}

Optionally use window.config if loading config dynamically.

â¸»

ğŸ§ª Step 3: Test Locally

yarn install
yarn start

Visit: http://localhost:3000

â¸»

ğŸ³ Step 4: Containerize Your MFE for Tutor

Create a Dockerfile in the root of frontend-app-ai-tutor:

FROM node:18 as builder
WORKDIR /app
COPY . .
RUN yarn install && yarn build

FROM nginx:alpine
COPY --from=builder /app/build /usr/share/nginx/html
COPY nginx.conf /etc/nginx/conf.d/default.conf

Create nginx.conf:

server {
    listen 80;
    location / {
        root   /usr/share/nginx/html;
        index  index.html index.htm;
        try_files $uri /index.html;
    }
}



â¸»

ğŸ“¦ Step 5: Create Tutor MFE Plugin

Go to Tutor plugin folder:

tutor plugins new ai-tutor
cd "$(tutor plugins printroot)/ai-tutor"

Create ai-tutor/plugin.py:

from tutor import hooks

hooks.Filters.MFE_APPS.add_item((
    "ai-tutor",
    {
        "repository": "local",
        "port": 3001,
        "url_path": "/ai-tutor",
    }
))



â¸»

ğŸ“ Step 6: Mount Your MFE Source for Build

Inside the plugin folder, create:

mkdir patches
mkdir mfe
cp -r ~/path-to/frontend-app-ai-tutor mfe/

Then update patches/docker-compose.override.yml:

services:
  mfe:
    volumes:
      - ./mfe/frontend-app-ai-tutor:/openedx/app:delegated



â¸»

ğŸš€ Step 7: Enable, Build, and Deploy

tutor plugins enable ai-tutor
tutor images build mfe
tutor local start -d

Visit:
ğŸ‘‰ http://local.edly.io/ai-tutor

â¸»

ğŸ”— Connect to Your FastAPI AI Backend

Use fetch("/tutor/assist", { method: "POST", ... }) in your React code to hit the FastAPI backend.

If the backend is running via Docker too:
	â€¢	Add a Docker network or use host.docker.internal on Mac/Windows
	â€¢	Or expose via tutor local do run nginx to map route externally

â¸»

ğŸ§  Bonus: Add Route to LMS Navigation

To add a link in LMS UI:
	â€¢	Override the LMS navbar template
	â€¢	Or use an MFE launch button via the account or learning MFEs

â¸»

Great! Letâ€™s connect your custom MFE to your FastAPI AI backend using secure token-based authentication, specifically using the JWT (OIDC) token from Open edX (via Azure AD B2C).

â¸»

ğŸ”’ Goal

When a learner visits /ai-tutor, the MFE should:
	1.	Retrieve their JWT token from the LMS (already logged in via Azure AD B2C)
	2.	Send the token in API requests to your FastAPI backend
	3.	Validate and extract user identity in FastAPI for personalized AI assistance

â¸»

ğŸ§± Prerequisites
	â€¢	Azure AD B2C is already set up with Tutor via tutor oauth or your custom IdP plugin
	â€¢	Your FastAPI backend is running (e.g., http://localhost:8001/tutor/assist)
	â€¢	You have a public key or issuer URL to validate JWTs (from Azure)

â¸»

âš›ï¸ 1. In the MFE: Retrieve and Send JWT

âœ… Step 1: Tutor automatically exposes window.APP_CONFIG.jwt if enabled.

In your MFE React component:

// utils/auth.ts
export const getJwtToken = () => {
  return (window.APP_CONFIG?.jwt || "").trim();
};

âœ… Step 2: Send it in your API requests

import { getJwtToken } from "./utils/auth";

const handleAsk = async () => {
  const token = getJwtToken();
  const response = await fetch("http://localhost:8001/tutor/assist", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${token}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify({ question }),
  });

  const data = await response.json();
  setAnswer(data.reply);
};

âœ”ï¸ Your request now includes an authenticated JWT.

â¸»

ğŸ§  2. In FastAPI: Decode and Verify JWT

Update your auth.py from earlier:

from fastapi import Header, HTTPException, Depends
from jose import jwt, JWTError
from pydantic import BaseModel
import requests
import os

OIDC_ISSUER = os.getenv("OIDC_ISSUER")
OIDC_CLIENT_ID = os.getenv("OIDC_CLIENT_ID")

class User(BaseModel):
    username: str
    email: str
    name: str = ""
    roles: list[str] = []

# Optional: Fetch keys from Azure if validating signature
def get_public_keys():
    jwks_uri = f"{OIDC_ISSUER}/discovery/v2.0/keys"
    return requests.get(jwks_uri).json()["keys"]

def get_current_user(authorization: str = Header(...)) -> User:
    token = authorization.replace("Bearer ", "")

    try:
        payload = jwt.decode(token, key="", options={"verify_signature": False})  # âš ï¸ Replace with actual key if validating
        if payload.get("aud") != OIDC_CLIENT_ID:
            raise HTTPException(status_code=401, detail="Invalid audience")

        return User(
            username=payload.get("sub"),
            email=payload.get("email"),
            name=payload.get("name", ""),
            roles=payload.get("roles", []),
        )
    except JWTError:
        raise HTTPException(status_code=401, detail="Invalid token")

âœ… In production, validate the tokenâ€™s signature using Azureâ€™s public keys from their JWKS URI. You can use jwcrypto or proper key loading via jose.

â¸»

ğŸ” 3. Use the User Object in Your AI Endpoint

@router.post("/assist")
async def tutor_assist(request: Request, user: User = Depends(get_current_user)):
    data = await request.json()
    question = data.get("question", "")
    
    if not question:
        raise HTTPException(400, "Missing question")

    reply = get_personalized_help(user, question)
    return {"reply": reply}



â¸»

ğŸ” 4. Test the Flow
	1.	Log in as a learner in Open edX
	2.	Visit /ai-tutor MFE
	3.	Ask a question
	4.	The request should go:
	â€¢	With JWT â†’ to FastAPI â†’ verified â†’ AI replies personalized to user

â¸»

ğŸ”§ Optional: Tutor Config to Enable jwt in APP_CONFIG

If window.APP_CONFIG.jwt is missing:

tutor config save --set JWT_AUTH=true
tutor local start -d



â¸»

ğŸ§ª Test from Browser Dev Console

window.APP_CONFIG.jwt



â¸»

âœ… Final Flow Recap

[MFE React App]
â†“ uses window.APP_CONFIG.jwt
â†“ sends token via fetch
â†“
[FastAPI Backend]
â†’ decodes JWT
â†’ gets user info (email, name, roles)
â†’ calls AI model with context
â†’ returns personalized reply



â¸»

Absolutely! If you want your custom AI Tutor MFE to feel native to Open edX (i.e., accessible via LMS/Studio navigation, permissions-aware, and secure), hereâ€™s a step-by-step guide on how to override the LMS/Studio UI to natively integrate your MFE using Tutor.

â¸»

ğŸ¯ Goal

âœ… Add a new navigation entry (e.g., â€œAI Tutorâ€)
âœ… Securely route users to /ai-tutor
âœ… Make it look and behave like a first-class feature

â¸»

ğŸ› ï¸ Step 1: Create a Django LMS Template Override

Tutor makes this easy using a custom theme or the openedx plugin system.

Option A: Using Tutor openedx plugin
	1.	Enable it:

tutor plugins enable openedx


	2.	Create override path:

mkdir -p "$(tutor config printroot)/env/build/openedx/themes/mytheme/lms/templates/navigation"


	3.	Copy and override the LMS navigation bar template:
File:
$(tutor config printroot)/env/build/openedx/themes/mytheme/lms/templates/navigation/navbar-authenticated.html
Add your custom menu entry inside the <ul>:

<li class="nav-item">
  <a class="nav-link" href="/ai-tutor" target="_self">
    <i class="fa fa-robot"></i> AI Tutor
  </a>
</li>



â¸»

ğŸ§° Step 2: Register the Theme and Rebuild

In your tutor config save --set:

tutor config save --set THEME_NAME=mytheme

Then rebuild Open edX to apply your theme:

tutor images build openedx
tutor local start -d



â¸»

ğŸ” Step 3: Secure the MFE Route

If your FastAPI backend uses token-based auth:
	1.	The LMS or MFE must pass the JWT or session data.
	2.	If using Open edX JWT auth:
	â€¢	Extract the token from request.user.access_token
	â€¢	Or expose /jwt/api/token and let MFE fetch token
	3.	Configure MFE or FastAPI backend to validate JWT tokens issued by LMS or Azure B2C.

Optionally, add a custom Django view for routing /ai-tutor with token injection logic.

â¸»

âš™ï¸ Step 4: Add Custom Django View for /ai-tutor (Optional)

If you want to insert tokens/session/etc. into the frontend:
	1.	Create a urls.py in LMS overrides:

# lms/urls.py
from django.urls import path
from django.shortcuts import redirect

def launch_ai_tutor(request):
    # Optionally inject user info or token
    return redirect("/ai-tutor")

urlpatterns += [
    path("ai-tutor/", launch_ai_tutor, name="ai_tutor"),
]

	2.	Register this view using tutor patching method in openedx plugin:

tutor config save --set ENABLE_IA_TUTOR=True


	3.	Restart LMS:

tutor local run lms bash
pkill -HUP gunicorn



â¸»

ğŸ¨ Optional: Customize Studio (CMS)

You can do the same thing in Studio:
	1.	Override Studio nav bar:

mkdir -p "$(tutor config printroot)/env/build/openedx/themes/mytheme/cms/templates/navigation"
cp the `navbar.html` from edx-platform/cms/templates/navigation/


	2.	Add a link to /ai-tutor or /course-authoring/ai-tutor.

â¸»

âœ… Final Result
	â€¢	ğŸ§  Users see a new nav entry â€œAI Tutorâ€ in LMS and Studio.
	â€¢	ğŸ” Route uses real authentication from LMS via JWT or session.
	â€¢	ğŸ¤– MFE loads from /ai-tutor and talks to FastAPI securely.
	â€¢	ğŸ§© Feels like a native extension to Open edX.

â¸»

```
# ai_tutor_plugin/plugin.py
from tutor import hooks

# Add a menu item in the LMS navigation bar using a theme override
hooks.Filters.ENV_PATCHES.add_items([
    ("lms-env", {
        "ENABLE_AI_TUTOR": True,
    }),
])

# Add the MFE as a microfrontend app served by Tutor
hooks.Filters.MFE_APPS.add_item((
    "ai-tutor",
    {
        "repository": "local",
        "port": 3001,
        "url_path": "/ai-tutor",
    }
))

# Add custom URL route for AI Tutor launcher view
@hooks.Filters.LMS_URLS.add()
def add_ai_tutor_urlpatterns():
    from django.urls import path
    from django.shortcuts import redirect

    def launch_ai_tutor(request):
        # You can add logging, token injection, or other logic here
        return redirect("/ai-tutor")

    return [
        path("ai-tutor/", launch_ai_tutor, name="ai_tutor_launcher"),
    ]
```

âœ… Iâ€™ve created a ready-to-use Tutor plugin template that:
	â€¢	Adds your custom AI Tutor MFE (/ai-tutor) to the LMS
	â€¢	Enables native route handling with a custom Django view
	â€¢	Sets up your MFE inside Tutorâ€™s MFE infrastructure

â¸»

ğŸ”§ What You Should Do Next
	1.	Create the plugin folder:

tutor plugins new ai-tutor


	2.	Replace the contents of plugin.py with the code above from â€œAi Tutor Pluginâ€.
	3.	Copy your built MFE (frontend-app-ai-tutor) into:

$(tutor plugins printroot)/ai-tutor/mfe/frontend-app-ai-tutor


	4.	Enable and rebuild:

tutor plugins enable ai-tutor
tutor images build mfe
tutor local start -d


	5.	Test the route:
Go to http://local.edly.io/ai-tutor from the LMS.

â¸»

Perfect! Hereâ€™s how to add â€œAI Tutorâ€ to the LMS navigation bar using a Tutor theme override.

â¸»

ğŸ¨ LMS Navigation Bar Override

ğŸ“ Step 1: Create override directory

mkdir -p "$(tutor config printroot)/env/build/openedx/themes/mytheme/lms/templates/navigation"

Replace mytheme with your actual theme name if it differs.

â¸»

ğŸ“ Step 2: Copy and edit the LMS navigation bar

Create a file:

$(tutor config printroot)/env/build/openedx/themes/mytheme/lms/templates/navigation/navbar-authenticated.html

Paste the following content (based on Open edXâ€™s default, with our addition â¬‡ï¸):

{% include "navigation/navbar-logo.html" %}
{% include "navigation/navbar-catalog.html" %}

<ul class="nav navbar-nav navbar-right">
  <!-- Existing nav items -->
  {% include "navigation/navbar-user-menu.html" %}

  <!-- âœ… Custom AI Tutor Nav Item -->
  <li class="nav-item">
    <a class="nav-link" href="/ai-tutor">
      <i class="fa fa-robot"></i> AI Tutor
    </a>
  </li>
</ul>



â¸»

âš™ï¸ Step 3: Set the theme in Tutor config

tutor config save --set THEME_NAME=mytheme



â¸»

ğŸ—ï¸ Step 4: Rebuild and restart LMS

tutor images build openedx
tutor local start -d



â¸»

âœ… Result

Youâ€™ll now see a menu item labeled â€œAI Tutorâ€ in the LMS navigation bar that links to your MFE (/ai-tutor) â€” making it feel fully native to Open edX!

â¸»

Would you also like to:
	â€¢	Add this menu to Studio (CMS)?
	â€¢	Dynamically hide it unless the user has a certain role (e.g., student/instructor)?
	â€¢	Or inject JWT tokens automatically for your MFE?

```
# ai_tutor_plugin/plugin.py
from tutor import hooks

# Enable configuration flag (optional)
hooks.Filters.ENV_PATCHES.add_items([
    ("lms-env", {
        "ENABLE_AI_TUTOR": True,
    }),
])

# Register MFE
hooks.Filters.MFE_APPS.add_item((
    "ai-tutor",
    {
        "repository": "local",
        "port": 3001,
        "url_path": "/ai-tutor",
    }
))

# LMS route override for custom launcher view
@hooks.Filters.LMS_URLS.add()
def add_ai_tutor_urlpatterns():
    from django.urls import path
    from django.shortcuts import redirect
    from django.contrib.auth.decorators import login_required
    import urllib.parse

    @login_required
    def launch_ai_tutor(request):
        token = getattr(request.user, 'access_token', None)
        target_url = "/ai-tutor"
        if token:
            target_url += f"?token={urllib.parse.quote(token)}"
        return redirect(target_url)

    return [
        path("ai-tutor/", launch_ai_tutor, name="ai_tutor_launcher"),
    ]

# Studio route override for custom launcher view
@hooks.Filters.CMS_URLS.add()
def add_ai_tutor_studio_urlpatterns():
    from django.urls import path
    from django.shortcuts import redirect
    from django.contrib.auth.decorators import login_required
    import urllib.parse

    @login_required
    def launch_ai_tutor_studio(request):
        token = getattr(request.user, 'access_token', None)
        target_url = "/ai-tutor"
        if token:
            target_url += f"?token={urllib.parse.quote(token)}"
        return redirect(target_url)

    return [
        path("ai-tutor/", launch_ai_tutor_studio, name="ai_tutor_launcher_studio"),
    ]
```

âœ… Done! Your plugin now supports full LMS & Studio integration, including:
	â€¢	Navigation route for both LMS and Studio
	â€¢	Auto-injects JWT token into the redirect URL (/ai-tutor?token=...)
	â€¢	Secured by @login_required

â¸»

ğŸ¨ Add AI Tutor to Studio (CMS) Navigation

Follow the same pattern as LMS.

ğŸ“ Step 1: Create the CMS override directory

mkdir -p "$(tutor config printroot)/env/build/openedx/themes/mytheme/cms/templates/navigation"



â¸»

ğŸ“ Step 2: Edit navbar.html

Create or edit this file:

$(tutor config printroot)/env/build/openedx/themes/mytheme/cms/templates/navigation/navbar.html

Add this inside the navigation bar <ul>:

<li class="nav-item">
  <a class="nav-link" href="/ai-tutor">
    <i class="fa fa-robot"></i> AI Tutor
  </a>
</li>



â¸»

ğŸ” Optional: Role-Based Visibility

To show this link only to specific roles (e.g., students or instructors), wrap the link in a Django conditional:

Example (LMS):

{% if user.is_staff or user.is_superuser %}
  <li class="nav-item">
    <a class="nav-link" href="/ai-tutor">
      <i class="fa fa-robot"></i> AI Tutor
    </a>
  </li>
{% endif %}

Or define a custom role check based on groups or enrollment.

â¸»

ğŸ”„ Summary of Whatâ€™s Done

Feature	âœ… Implemented
LMS nav override	âœ… Yes
Studio nav override	âœ… Yes
Route launcher for LMS	âœ… Yes
Route launcher for Studio	âœ… Yes
Token injection via redirect	âœ… Yes
Role-based visibility (opt-in)	âœ… Available



â¸»

Would you like:
	â€¢	A reusable GitHub repo with all this pre-packaged?
	â€¢	The MFE to auto-read the token from query and store it for API use?
	â€¢	Or preconfigured MFE project scaffold with working /assist call?

Happy to build out the next step with you!


Great! Hereâ€™s how to make your custom MFE (React-based) automatically:

âœ… Read the ?token=... from the URL
âœ… Store it (e.g., in localStorage)
âœ… Use it for authenticated API calls to your FastAPI AI backend

â¸»

âš›ï¸ 1. Auto-read Token from URL and Store It

Add this to your App.tsx or root-level useEffect():

import React, { useEffect } from "react";

const App: React.FC = () => {
  useEffect(() => {
    const params = new URLSearchParams(window.location.search);
    const token = params.get("token");

    if (token) {
      localStorage.setItem("ai_tutor_token", token);
      // Optionally clean URL (remove ?token=...)
      const cleanUrl = window.location.origin + window.location.pathname;
      window.history.replaceState({}, document.title, cleanUrl);
    }
  }, []);

  return (
    <div className="App">
      <h2>Welcome to AI Tutor</h2>
      {/* Your routes/components here */}
    </div>
  );
};

export default App;



â¸»

ğŸ“¡ 2. Use the Stored Token in API Requests

In your API service file (e.g. api.ts):

export async function askAI(question: string): Promise<string> {
  const token = localStorage.getItem("ai_tutor_token");

  const res = await fetch("/tutor/assist", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      ...(token && { Authorization: `Bearer ${token}` }),
    },
    body: JSON.stringify({ question }),
  });

  const data = await res.json();
  return data.reply || "No answer received.";
}



â¸»

ğŸ§ª 3. Test the Flow
	1.	Visit:
http://local.edly.io/ai-tutor?token=eyJ...
	2.	You should:
	â€¢	Be redirected to /ai-tutor
	â€¢	See the token is stored
	â€¢	Successfully call your backend with it

â¸»

ğŸ›¡ï¸ Best Practices

Topic	Recommendation
Token security	Use sessionStorage instead of localStorage if possible
Token lifespan	Respect token expiration and renew if needed
Refresh flow	Add silent refresh logic (OIDC or LMS JWT)
HTTPS	Always run over HTTPS in production



â¸»

âœ… Final Result

Your MFE now:
	â€¢	Auto-reads and stores tokens
	â€¢	Sends them securely to your AI backend
	â€¢	Is production-ready for Open edX + FastAPI integrations

â¸»

Would you like this MFE bootstrapped as a GitHub repo or template with:
	â€¢	Token handling prebuilt
	â€¢	Configurable API URL
	â€¢	A working AI chat UI component?

Happy to zip that up or push it to GitHub for you.